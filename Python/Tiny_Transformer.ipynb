{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Baseline Model**"
      ],
      "metadata": {
        "id": "jyNQdNSVp2gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Importing Libraries\n",
        "# =============================================================================\n",
        "!pip install wfdb\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import h5py\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import os\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import wfdb\n",
        "import random\n",
        "import scipy.signal\n",
        "from scipy import signal\n",
        "from collections import Counter\n",
        "import json\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "8wHos1YRpyoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "class Config:\n",
        "    # Model hyperparameters\n",
        "    class_types = ['N', 'S', 'V', 'F', 'Q']\n",
        "    num_heads = 8\n",
        "    hidden_size = 2\n",
        "    embedding = 16\n",
        "    mlp_dim = 128\n",
        "    kernel = 3\n",
        "    emb_depth = 1\n",
        "    transformer_layers = 1\n",
        "    half_window = 99\n",
        "    batch_size = 128\n",
        "\n",
        "    # Training parameters\n",
        "    learning_rate = 2e-3\n",
        "    epochs = 100\n",
        "    patience = 25\n",
        "    random_seed = 42\n",
        "\n",
        "    # Data paths\n",
        "    data_path = 'physionet.org/files/mitdb/1.0.0/'\n",
        "    splits_file = 'consistent_data_splits.npz'\n",
        "    baseline_weights = 'baseline_ecg_transformer.weights.h5'\n",
        "    baseline_results = 'baseline_results.json'\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Set all random seeds for reproducibility\n",
        "np.random.seed(config.random_seed)\n",
        "tf.random.set_seed(config.random_seed)\n",
        "random.seed(config.random_seed)"
      ],
      "metadata": {
        "id": "E9bSTSyYqHCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparting Data and Dataset"
      ],
      "metadata": {
        "id": "f4N0dV2hk5FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ECG DATA PREPROCESSING\n",
        "# =============================================================================\n",
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = signal.butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = signal.lfilter(b, a, data)\n",
        "    return y\n",
        "\n",
        "def preprocess_ecg(signal_data, fs=360):\n",
        "    \"\"\"Preprocess ECG signal: bandpass filter 0.5-40 Hz + normalization\"\"\"\n",
        "    filtered = butter_bandpass_filter(signal_data, 0.5, 40.0, fs, order=5)\n",
        "    normalized = (filtered - np.mean(filtered)) / (np.std(filtered) + 1e-8)\n",
        "    return normalized\n",
        "\n",
        "def extract_beats(record_path, classes_to_use=None):\n",
        "    \"\"\"Extract beats from a single MIT-BIH record with proper AAMI mapping\"\"\"\n",
        "    if classes_to_use is None:\n",
        "        classes_to_use = config.class_types\n",
        "\n",
        "    # MIT-BIH to AAMI symbol mapping (AAMI EC57 standard)\n",
        "    mit_bih_to_aami = {\n",
        "        'N': 'N', 'L': 'N', 'R': 'N', 'e': 'N', 'j': 'N','B':'N',\n",
        "        'A': 'S', 'a': 'S', 'J': 'S', 'S': 'S',\n",
        "        'V': 'V', 'E': 'V',\n",
        "        'F': 'F',\n",
        "        '/': 'Q', 'f': 'Q', 'Q': 'Q',\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        record = wfdb.rdrecord(record_path)\n",
        "        annotation = wfdb.rdann(record_path, 'atr')\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {record_path}: {e}\")\n",
        "        return [], [], []\n",
        "\n",
        "    signal_data = record.p_signal[:, 0]\n",
        "    fs = record.fs\n",
        "    signal_data = preprocess_ecg(signal_data, fs)\n",
        "\n",
        "    beats, labels, rr_intervals = [], [], []\n",
        "    r_peaks = annotation.sample\n",
        "    symbols = annotation.symbol\n",
        "\n",
        "    for i in range(1, len(r_peaks) - 1):\n",
        "        mit_symbol = symbols[i]\n",
        "        if mit_symbol in mit_bih_to_aami:\n",
        "            aami_class = mit_bih_to_aami[mit_symbol]\n",
        "            if aami_class in classes_to_use:\n",
        "                start = r_peaks[i] - config.half_window\n",
        "                end = r_peaks[i] + config.half_window\n",
        "\n",
        "                if start < 0 or end >= len(signal_data):\n",
        "                    continue\n",
        "\n",
        "                beat = signal_data[start:end]\n",
        "\n",
        "                # Calculate RR intervals (normalized)\n",
        "                pre_rr = (r_peaks[i] - r_peaks[i-1]) / fs\n",
        "                post_rr = (r_peaks[i+1] - r_peaks[i]) / fs\n",
        "                pre_rr = np.clip(pre_rr, 0, 2)\n",
        "                post_rr = np.clip(post_rr, 0, 2)\n",
        "\n",
        "                beats.append(beat)\n",
        "                labels.append(aami_class)\n",
        "                rr_intervals.append([pre_rr, post_rr])\n",
        "\n",
        "    return beats, labels, rr_intervals"
      ],
      "metadata": {
        "id": "nSBLBthSqVVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Dataset load and split\n",
        "# =============================================================================\n",
        "def load_mit_bih_data():\n",
        "    \"\"\"Load complete MIT-BIH dataset\"\"\"\n",
        "\n",
        "    all_records = ['100', '101', '102', '103', '104', '105', '106', '107', '108', '109',\n",
        "                   '111', '112', '113', '114', '115', '116', '117', '118', '119', '121',\n",
        "                   '122', '123', '124', '200', '201', '202', '203', '205', '207', '208',\n",
        "                   '209', '210', '212', '213', '214', '215', '217', '219', '220', '221',\n",
        "                   '222', '223', '228', '230', '231', '232', '233', '234']\n",
        "\n",
        "    all_beats = []\n",
        "    all_labels = []\n",
        "    all_rr_intervals = []\n",
        "\n",
        "    print(f\"Loading MIT-BIH data from {config.data_path}...\")\n",
        "    for record in all_records:\n",
        "        try:\n",
        "            record_path = os.path.join(config.data_path, record)\n",
        "            beats, labels, rr_intervals = extract_beats(record_path)\n",
        "            all_beats.extend(beats)\n",
        "            all_labels.extend(labels)\n",
        "            all_rr_intervals.extend(rr_intervals)\n",
        "            print(f\"  ✓ Record {record}: {len(beats)} beats\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Record {record}: {e}\")\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_beats = np.array(all_beats)\n",
        "    all_rr_intervals = np.array(all_rr_intervals)\n",
        "\n",
        "    # Convert labels to integers\n",
        "    label_map = {label: idx for idx, label in enumerate(config.class_types)}\n",
        "    all_labels_int = np.array([label_map[label] for label in all_labels])\n",
        "\n",
        "    print(f\"\\nTotal beats loaded: {len(all_beats)}\")\n",
        "    print(f\"Label distribution: {Counter(all_labels)}\")\n",
        "\n",
        "    return all_beats, all_labels_int, all_rr_intervals\n",
        "\n",
        "def create_consistent_data_splits(force_recreate=False):\n",
        "    \"\"\"Create train/val/test splits\"\"\"\n",
        "    if os.path.exists(config.splits_file) and not force_recreate:\n",
        "        print(f\"Loading existing data splits from {config.splits_file}\")\n",
        "        data = np.load(config.splits_file)\n",
        "        return (data['X_train'], data['X_val'], data['X_test'],\n",
        "                data['y_train'], data['y_val'], data['y_test'],\n",
        "                data['RR_train'], data['RR_val'], data['RR_test'])\n",
        "\n",
        "    print(\"Creating data splits...\")\n",
        "\n",
        "    # Load complete dataset\n",
        "    beats, labels, rr_intervals = load_mit_bih_data()\n",
        "    if len(beats) == 0:\n",
        "        raise ValueError(\"No data loaded! Check your data path.\")\n",
        "\n",
        "    # Step 1: 70% train, 30% temp (validation + test)\n",
        "    X_train, X_temp, y_train, y_temp, RR_train, RR_temp = train_test_split(\n",
        "        beats, labels, rr_intervals,\n",
        "        test_size=0.30,\n",
        "        random_state=config.random_seed,\n",
        "        stratify=labels\n",
        "    )\n",
        "\n",
        "    # Step 2: Split temp into 10% validation, 20% test\n",
        "    X_val, X_test, y_val, y_test, RR_val, RR_test = train_test_split(\n",
        "        X_temp, y_temp, RR_temp,\n",
        "        test_size=2/3,\n",
        "        random_state=config.random_seed,\n",
        "        stratify=y_temp\n",
        "    )\n",
        "\n",
        "    # Print split distributions\n",
        "    def print_split_info(y, split_name):\n",
        "        unique, counts = np.unique(y, return_counts=True)\n",
        "        total = len(y)\n",
        "        print(f\"\\n{split_name} ({total} samples, {total/len(labels)*100:.1f}%):\")\n",
        "        for class_idx, count in zip(unique, counts):\n",
        "            class_name = config.class_types[class_idx]\n",
        "            percentage = (count / total) * 100\n",
        "            print(f\"  {class_name}: {count:5d} ({percentage:5.1f}%)\")\n",
        "\n",
        "    print_split_info(y_train, \"Training\")\n",
        "    print_split_info(y_val, \"Validation\")\n",
        "    print_split_info(y_test, \"Test\")\n",
        "\n",
        "    # Save splits for consistency across experiments\n",
        "    np.savez(config.splits_file,\n",
        "             X_train=X_train, X_val=X_val, X_test=X_test,\n",
        "             y_train=y_train, y_val=y_val, y_test=y_test,\n",
        "             RR_train=RR_train, RR_val=RR_val, RR_test=RR_test)\n",
        "\n",
        "    print(f\"Data splits saved to {config.splits_file}\")\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, RR_train, RR_val, RR_test"
      ],
      "metadata": {
        "id": "Tbnp4WNbn8_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# DATASET CREATION\n",
        "# =============================================================================\n",
        "def create_datasets(X_train, RR_train, y_train, X_val, RR_val, y_val, batch_size):\n",
        "    \"\"\"Create TensorFlow datasets for training\"\"\"\n",
        "    autotune = tf.data.AUTOTUNE\n",
        "\n",
        "    # Reshape for Conv2D\n",
        "    X_train = X_train.reshape(-1, 198, 1, 1)\n",
        "    X_val = X_val.reshape(-1, 198, 1, 1)\n",
        "\n",
        "    # Convert to categorical\n",
        "    y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=len(config.class_types))\n",
        "    y_val_cat = tf.keras.utils.to_categorical(y_val, num_classes=len(config.class_types))\n",
        "\n",
        "    # Training dataset\n",
        "    train_data = tf.data.Dataset.from_tensor_slices(X_train)\n",
        "    train_RR = tf.data.Dataset.from_tensor_slices(RR_train)\n",
        "    train_labels = tf.data.Dataset.from_tensor_slices(y_train_cat)\n",
        "\n",
        "    X_train_ds = tf.data.Dataset.zip((train_data, train_RR)).map(\n",
        "        lambda x1, x2: {'x1': x1, 'x2': x2}\n",
        "    )\n",
        "    train_ds = tf.data.Dataset.zip((X_train_ds, train_labels))\n",
        "    train_ds = (train_ds.shuffle(10000, seed=config.random_seed)\n",
        "                .batch(batch_size)\n",
        "                .prefetch(autotune))\n",
        "\n",
        "    # Validation dataset\n",
        "    val_data = tf.data.Dataset.from_tensor_slices(X_val)\n",
        "    val_RR = tf.data.Dataset.from_tensor_slices(RR_val)\n",
        "    val_labels = tf.data.Dataset.from_tensor_slices(y_val_cat)\n",
        "\n",
        "    X_val_ds = tf.data.Dataset.zip((val_data, val_RR)).map(\n",
        "        lambda x1, x2: {'x1': x1, 'x2': x2}\n",
        "    )\n",
        "    val_ds = tf.data.Dataset.zip((X_val_ds, val_labels))\n",
        "    val_ds = (val_ds.batch(batch_size)\n",
        "              .prefetch(autotune))\n",
        "\n",
        "    return train_ds, val_ds\n",
        "\n",
        "def create_test_dataset(X_test, RR_test, y_test, batch_size):\n",
        "    \"\"\"Create test dataset\"\"\"\n",
        "    X_test = X_test.reshape(-1, 198, 1, 1)\n",
        "    y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes=len(config.class_types))\n",
        "\n",
        "    test_data = tf.data.Dataset.from_tensor_slices(X_test)\n",
        "    test_RR = tf.data.Dataset.from_tensor_slices(RR_test)\n",
        "    test_labels = tf.data.Dataset.from_tensor_slices(y_test_cat)\n",
        "\n",
        "    X_test_ds = tf.data.Dataset.zip((test_data, test_RR)).map(\n",
        "        lambda x1, x2: {'x1': x1, 'x2': x2}\n",
        "    )\n",
        "    test_ds = tf.data.Dataset.zip((X_test_ds, test_labels))\n",
        "    test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return test_ds"
      ],
      "metadata": {
        "id": "076y5hZKkz1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Build-Training-Test-Visualization"
      ],
      "metadata": {
        "id": "PR0nYREclRn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# MODEL COMPONENTS\n",
        "# =============================================================================\n",
        "def generate_patch_conv_orgPaper_f(embedding, inputs):\n",
        "    patches = layers.Conv2D(filters=embedding, kernel_size=[config.kernel,1],\n",
        "                           strides=[config.kernel,1], padding='valid')(inputs)\n",
        "    for _ in range(config.emb_depth - 1):\n",
        "        patches = layers.BatchNormalization()(patches)\n",
        "        patches = layers.Conv2D(filters=embedding, kernel_size=[config.kernel,1],\n",
        "                               strides=[config.kernel,1], padding='valid')(patches)\n",
        "\n",
        "    row_axis, col_axis = (1, 2)\n",
        "    seq_len = (patches.shape[row_axis]) * (patches.shape[col_axis])\n",
        "    x = layers.Reshape((seq_len, embedding))(patches)\n",
        "    return x\n",
        "\n",
        "class AddPositionEmbs(layers.Layer):\n",
        "    def __init__(self, posemb_init=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.posemb_init = posemb_init\n",
        "\n",
        "    def build(self, inputs_shape):\n",
        "        pos_emb_shape = (1, inputs_shape[1], inputs_shape[2])\n",
        "        self.pos_embedding = self.add_weight(\n",
        "            name='pos_embedding',\n",
        "            shape=pos_emb_shape,\n",
        "            initializer=self.posemb_init,\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, inputs_positions=None):\n",
        "        pos_embedding = tf.cast(self.pos_embedding, inputs.dtype)\n",
        "        return inputs + pos_embedding\n",
        "\n",
        "def mlp_block_f(mlp_dim, inputs):\n",
        "    x = layers.Dense(units=mlp_dim, activation=tf.nn.gelu)(inputs)\n",
        "    x = layers.Dropout(rate=0.1)(x)\n",
        "    x = layers.Dense(units=inputs.shape[-1], activation=tf.nn.gelu)(x)\n",
        "    x = layers.Dropout(rate=0.1)(x)\n",
        "    return x\n",
        "\n",
        "def BaselineEncoder1Dblock(num_heads, mlp_dim, inputs):\n",
        "    \"\"\"Standard transformer block with fixed multi-head attention\"\"\"\n",
        "    x = layers.LayerNormalization(dtype=inputs.dtype)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=config.hidden_size,\n",
        "        dropout=0.1\n",
        "    )(x, x)\n",
        "    x = layers.Add()([x, inputs])\n",
        "\n",
        "    y = layers.LayerNormalization(dtype=x.dtype)(x)\n",
        "    y = mlp_block_f(mlp_dim, y)\n",
        "    y_1 = layers.Add()([y, x])\n",
        "    return y_1\n",
        "\n",
        "def build_baseline_ViT():\n",
        "    \"\"\"Build baseline Vision Transformer (standard attention)\"\"\"\n",
        "    inputs = layers.Input(shape=(198,1,1), name='x1')\n",
        "    RR_feat = layers.Input(shape=(2,), name='x2')\n",
        "\n",
        "    patches = generate_patch_conv_orgPaper_f(config.embedding, inputs)\n",
        "    x = AddPositionEmbs(\n",
        "        posemb_init=tf.keras.initializers.RandomNormal(stddev=0.02),\n",
        "        name='posembed_input'\n",
        "    )(patches)\n",
        "    x = layers.Dropout(rate=0.2)(x)\n",
        "\n",
        "    x = BaselineEncoder1Dblock(config.num_heads, config.mlp_dim, x)\n",
        "    encoded = layers.LayerNormalization(name='encoder_norm')(x)\n",
        "    im_representation = layers.GlobalAveragePooling1D()(encoded)\n",
        "\n",
        "    emb_RR = layers.Dense(units=2, use_bias=False)(RR_feat)\n",
        "    im_representation = layers.Concatenate()([im_representation, emb_RR])\n",
        "\n",
        "    logits = layers.Dense(\n",
        "        units=len(config.class_types),\n",
        "        name='head',\n",
        "        kernel_initializer=tf.keras.initializers.zeros()\n",
        "    )(im_representation)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[inputs, RR_feat], outputs=logits)\n",
        "    return model"
      ],
      "metadata": {
        "id": "-8Qe7lBKqX9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# TRAINING FUNCTION\n",
        "# =============================================================================\n",
        "def train_baseline_model(model, train_ds, val_ds, weights_path):\n",
        "    \"\"\"Train baseline model\"\"\"\n",
        "    print(f\"\\n Training Baseline ECG Transformer...\")\n",
        "\n",
        "    # Compile model\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Callbacks\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        weights_path,\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=config.patience,\n",
        "        restore_best_weights=True,\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=10,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        epochs=config.epochs,\n",
        "        validation_data=val_ds,\n",
        "        callbacks=[checkpoint, early_stopping, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(f\"✅ Baseline training completed!\")\n",
        "    print(f\"Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "ZRDjIJk7qi3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# EVALUATION FUNCTION\n",
        "# =============================================================================\n",
        "def evaluate_baseline_model(model, test_ds, y_test):\n",
        "    \"\"\"Evaluate baseline model comprehensively\"\"\"\n",
        "    print(f\"\\n Evaluating Baseline Model...\")\n",
        "\n",
        "    # Compile model for evaluation\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=config.learning_rate),\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Collect predictions\n",
        "    logits_list = []\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    print(\"Running inference...\")\n",
        "    for batch_idx, batch in enumerate(test_ds):\n",
        "        inputs_dict, labels_onehot = batch\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(inputs_dict, training=False)\n",
        "        logits_np = logits.numpy()\n",
        "        logits_list.append(logits_np)\n",
        "\n",
        "        # Calculate metrics\n",
        "        bsize = logits_np.shape[0]\n",
        "        total_samples += bsize\n",
        "\n",
        "        batch_loss = loss_fn(labels_onehot, logits).numpy()\n",
        "        total_loss += batch_loss * bsize\n",
        "\n",
        "        preds_int = np.argmax(logits_np, axis=1)\n",
        "        true_int = np.argmax(labels_onehot.numpy(), axis=1)\n",
        "        total_correct += np.sum(preds_int == true_int)\n",
        "\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"  Processed {batch_idx + 1} batches...\")\n",
        "\n",
        "    # Final metrics\n",
        "    test_loss = total_loss / total_samples\n",
        "    test_accuracy = total_correct / total_samples\n",
        "\n",
        "    # Get predictions and probabilities\n",
        "    all_logits = np.concatenate(logits_list, axis=0)\n",
        "    y_pred_probs = tf.nn.softmax(all_logits, axis=1).numpy()\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    # Detailed metrics with zero_division parameter\n",
        "    precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    recall_macro = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "    precision_weighted = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    recall_weighted = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "    print(f\"\\n Baseline Model Test Results:\")\n",
        "    print(f\"Test Loss:           {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy:       {test_accuracy:.4f}\")\n",
        "    print(f\"Precision (macro):   {precision_macro:.4f}\")\n",
        "    print(f\"Recall (macro):      {recall_macro:.4f}\")\n",
        "    print(f\"F1-Score (macro):    {f1_macro:.4f}\")\n",
        "    print(f\"Precision (weighted): {precision_weighted:.4f}\")\n",
        "    print(f\"Recall (weighted):   {recall_weighted:.4f}\")\n",
        "    print(f\"F1-Score (weighted): {f1_weighted:.4f}\")\n",
        "\n",
        "    # Classification report\n",
        "    print(f\"\\n Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=config.class_types, digits=4))\n",
        "\n",
        "    results = {\n",
        "        'model_name': 'Baseline',\n",
        "        'test_accuracy': float(test_accuracy),\n",
        "        'test_loss': float(test_loss),\n",
        "        'precision_macro': float(precision_macro),\n",
        "        'recall_macro': float(recall_macro),\n",
        "        'f1_macro': float(f1_macro),\n",
        "        'precision_weighted': float(precision_weighted),\n",
        "        'recall_weighted': float(recall_weighted),\n",
        "        'f1_weighted': float(f1_weighted),\n",
        "        'predictions': y_pred.tolist(),\n",
        "        'true_labels': y_test.tolist()\n",
        "    }\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "QyT32gFrqq00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# VISUALIZATION FUNCTIONS\n",
        "# =============================================================================\n",
        "def plot_baseline_training_history(history, save_path='baseline_training_history.png'):\n",
        "    \"\"\"Plot baseline training history\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "    # Loss plots\n",
        "    axes[0, 0].plot(history.history['loss'], label='Train Loss')\n",
        "    axes[0, 0].set_title('Training Loss')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True)\n",
        "\n",
        "    axes[0, 1].plot(history.history['val_loss'], label='Val Loss', color='orange')\n",
        "    axes[0, 1].set_title('Validation Loss')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "\n",
        "    # Accuracy plots\n",
        "    axes[1, 0].plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    axes[1, 0].set_title('Training Accuracy')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Accuracy')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True)\n",
        "\n",
        "    axes[1, 1].plot(history.history['val_accuracy'], label='Val Accuracy', color='orange')\n",
        "    axes[1, 1].set_title('Validation Accuracy')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Accuracy')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_baseline_confusion_matrix(results, save_path='baseline_confusion_matrix.png'):\n",
        "    \"\"\"Plot confusion matrix for baseline model\"\"\"\n",
        "    y_true = results['true_labels']\n",
        "    y_pred = results['predictions']\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "               xticklabels=config.class_types, yticklabels=config.class_types)\n",
        "    plt.title(f'Baseline Model Confusion Matrix\\nAccuracy: {results[\"test_accuracy\"]:.4f}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def download_mitbih_data():\n",
        "    \"\"\"Download MIT-BIH dataset if not present\"\"\"\n",
        "    if not os.path.exists(config.data_path):\n",
        "        print(\"📥 Downloading MIT-BIH Arrhythmia Database...\")\n",
        "        os.system('wget -r -N -c -np https://physionet.org/files/mitdb/1.0.0/')\n",
        "        print(\"✅ Download completed!\")\n",
        "    else:\n",
        "        print(\"✅ MIT-BIH data already exists.\")"
      ],
      "metadata": {
        "id": "5niEcE0Squ84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# MAIN EXECUTION\n",
        "# =============================================================================\n",
        "def main():\n",
        "    \"\"\"Main execution for baseline model\"\"\"\n",
        "    print(\" BASELINE ECG Transformer - Training & Evaluation\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Setup GPU if available\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            print(f\" Found {len(gpus)} GPU(s), memory growth enabled\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"GPU setup error: {e}\")\n",
        "    else:\n",
        "        print(\" No GPU found, running on CPU\")\n",
        "\n",
        "    # Download data if needed\n",
        "    download_mitbih_data()\n",
        "\n",
        "    # Create consistent data splits\n",
        "    print(\"\\n Creating/Loading data splits...\")\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test, RR_train, RR_val, RR_test = create_consistent_data_splits()\n",
        "\n",
        "    # Calculate class weights\n",
        "    class_weights = class_weight.compute_class_weight(\n",
        "        'balanced', classes=np.unique(y_train), y=y_train\n",
        "    )\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "    print(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_ds, val_ds = create_datasets(X_train, RR_train, y_train, X_val, RR_val, y_val, config.batch_size)\n",
        "    test_ds = create_test_dataset(X_test, RR_test, y_test, config.batch_size)\n",
        "\n",
        "    # Build and train baseline model\n",
        "    print(\"\\n🏗️  Building Baseline Model...\")\n",
        "    baseline_model = build_baseline_ViT()\n",
        "    baseline_model.summary()\n",
        "    print(f\"Baseline model parameters: {baseline_model.count_params():,}\")\n",
        "\n",
        "    # Check if model already exists\n",
        "    if os.path.exists(config.baseline_weights):\n",
        "        print(f\"\\n Found existing baseline weights: {config.baseline_weights}\")\n",
        "        choice = input(\"Do you want to (1) Retrain, (2) Load existing, or (3) Skip training? [1/2/3]: \")\n",
        "\n",
        "        if choice == '2':\n",
        "            print(\" Loading existing baseline weights...\")\n",
        "            baseline_model.load_weights(config.baseline_weights)\n",
        "            history = None\n",
        "        elif choice == '3':\n",
        "            print(\"Skipping baseline training...\")\n",
        "            return\n",
        "        else:\n",
        "            print(\" Retraining baseline model...\")\n",
        "            history = train_baseline_model(baseline_model, train_ds, val_ds, config.baseline_weights)\n",
        "            baseline_model.load_weights(config.baseline_weights)\n",
        "    else:\n",
        "        history = train_baseline_model(baseline_model, train_ds, val_ds, config.baseline_weights)\n",
        "        baseline_model.load_weights(config.baseline_weights)\n",
        "\n",
        "    # Evaluate baseline model\n",
        "    print(\"\\n Evaluating Baseline Model...\")\n",
        "    results = evaluate_baseline_model(baseline_model, test_ds, y_test)\n",
        "\n",
        "    # Save results\n",
        "    with open(config.baseline_results, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(f\" Results saved to {config.baseline_results}\")\n",
        "\n",
        "    # Generate visualizations\n",
        "    if history is not None:\n",
        "        print(\"\\n Generating training history plot...\")\n",
        "        plot_baseline_training_history(history)\n",
        "\n",
        "    print(\" Generating confusion matrix...\")\n",
        "    plot_baseline_confusion_matrix(results)\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🎉 BASELINE MODEL SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Test Accuracy:     {results['test_accuracy']:.4f}\")\n",
        "    print(f\"Test Loss:         {results['test_loss']:.4f}\")\n",
        "    print(f\"F1-Score (macro):  {results['f1_macro']:.4f}\")\n",
        "    print(f\"F1-Score (weighted): {results['f1_weighted']:.4f}\")\n",
        "\n",
        "    print(f\"\\n Files Generated:\")\n",
        "    print(f\"  - {config.baseline_weights} (model weights)\")\n",
        "    print(f\"  - {config.baseline_results} (detailed results)\")\n",
        "    print(f\"  - {config.splits_file} (data splits for consistency)\")\n",
        "    print(f\"  - baseline_training_history.png\")\n",
        "    print(f\"  - baseline_confusion_matrix.png\")\n",
        "\n",
        "    print(f\"\\n Baseline model training and evaluation completed!\")\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Install required package if needed\n",
        "    try:\n",
        "        import wfdb\n",
        "    except ImportError:\n",
        "        print(\"Installing wfdb package...\")\n",
        "        os.system('pip install wfdb')\n",
        "        import wfdb\n",
        "\n",
        "    # Run baseline pipeline\n",
        "    results = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C8Z0EcZEAZjA",
        "outputId": "825d71c8-1eba-4966-cce3-dfb1c2dbb168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " BASELINE ECG Transformer - Training & Evaluation\n",
            "============================================================\n",
            " Found 1 GPU(s), memory growth enabled\n",
            "✅ MIT-BIH data already exists.\n",
            "\n",
            " Creating/Loading data splits...\n",
            "Loading existing data splits from consistent_data_splits.npz\n",
            "Class weights: {0: np.float64(0.22354713868798376), 1: np.float64(7.246375321336761), 2: np.float64(2.872849571952711), 3: np.float64(25.123351158645278), 4: np.float64(1281.290909090909)}\n",
            "\n",
            "🏗️  Building Baseline Model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ x1 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m198\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m16\u001b[0m) │         \u001b[38;5;34m64\u001b[0m │ x1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ posembed_input      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │      \u001b[38;5;34m1,056\u001b[0m │ reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mAddPositionEmbs\u001b[0m)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ posembed_input[\u001b[38;5;34m0\u001b[0m… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │         \u001b[38;5;34m32\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │      \u001b[38;5;34m1,088\u001b[0m │ layer_normalizat… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
              "│                     │                   │            │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │         \u001b[38;5;34m32\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │      \u001b[38;5;34m2,176\u001b[0m │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │      \u001b[38;5;34m2,064\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_3 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                     │                   │            │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_norm        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m, \u001b[38;5;34m16\u001b[0m)    │         \u001b[38;5;34m32\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ x2 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ encoder_norm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m4\u001b[0m │ x2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ head (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │         \u001b[38;5;34m95\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ x1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>) │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ x1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ posembed_input      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │ reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AddPositionEmbs</span>)   │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ posembed_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,088</span> │ layer_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
              "│                     │                   │            │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,176</span> │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,064</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                     │                   │            │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_norm        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ x2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_norm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │ x2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ head (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">95</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,643\u001b[0m (25.95 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,643</span> (25.95 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,643\u001b[0m (25.95 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,643</span> (25.95 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline model parameters: 6,643\n",
            "\n",
            " Found existing baseline weights: baseline_ecg_transformer.weights.h5\n",
            "Do you want to (1) Retrain, (2) Load existing, or (3) Skip training? [1/2/3]: 3\n",
            "Skipping baseline training...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "_8tx7pa3rNuO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WOnyH54yhV8a"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}